package org.tensorframes.impl

import org.apache.commons.lang3.SerializationUtils
import org.apache.spark.{LoggingWrapper => Logging}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.sql.catalyst.expressions.{MutableRow, GenericRowWithSchema}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.{RelationalGroupedDataset, DataFrame, Row}
import org.bytedeco.javacpp.{tensorflow => jtf}
import org.tensorflow.framework.GraphDef
import org.tensorframes._
import org.tensorframes.test.DslOperations

import scala.collection.mutable
import scala.util.{Failure, Success, Try}

/**
 * A simple and slow implementation of the basic operations that maximizes correctness of
 * implementation and works with older versions of Spark (based on RDDs).
 */
class DebugRowOps
  extends OperationsInterface
    with ExperimentalOperations with DslOperations with PythonInterface with Logging {

  import SchemaTransforms._

  override def toString: String = "DebugRowOps"

  override def mapBlocks(
      dataframe: DataFrame,
      graph: GraphDef,
      shapeHints: ShapeDescription): DataFrame = {
    mapBlocks(dataframe, graph, shapeHints, appendInput = true)
  }


  override def mapBlocksTrimmed(
      dataframe: DataFrame,
      graph: GraphDef,
      shapeHints: ShapeDescription): DataFrame = {
    mapBlocks(dataframe, graph, shapeHints, appendInput = false)
  }

  private def mapBlocks(
      dataframe: DataFrame,
      graph: GraphDef,
      shapeHints: ShapeDescription,
      appendInput: Boolean): DataFrame = {
    val sc = dataframe.sqlContext.sparkContext
    val transform = mapBlocksSchema(dataframe.schema, graph, shapeHints, appendInput)

    logDebug(s"mapBlocks: TF input schema = ${transform.inputSchema}," +
      s" complete output schema = ${transform.outputSchema}")

    val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))
    val transformRdd = dataframe.rdd.mapPartitions { it =>
      // If the partition is empty, return immediately, it seems to happen often for small datasets
      // TODO(tjh) add test for that
      if (it.hasNext) {
        DebugRowOpsImpl.performMap(
          it.toArray,
          transform.inputSchema,
          transform.mapTFCols,
          gProto.value,
          transform.outputTFSchema,
          appendInput)
      } else {
        mutable.Iterable.empty.iterator
      }
    }
    dataframe.sqlContext.createDataFrame(transformRdd, transform.outputSchema)
  }


  override def mapRows(
      dataframe: DataFrame,
      graph: GraphDef,
      shapeHints: ShapeDescription): DataFrame = {
    val sc = dataframe.sqlContext.sparkContext
    val summary = TensorFlowOps.analyzeGraph(graph, shapeHints)
      .map(x => x.name -> x).toMap
    val inputs = summary.filter(_._2.isInput)
    val outputs = summary.filter(_._2.isOutput)
    val fieldsByName = dataframe.schema.fields.map(f => f.name -> f).toMap
    val cols = dataframe.schema.fieldNames.mkString(", ")

    inputs.values.foreach { in =>
      val f = get(fieldsByName.get(in.name),
        s"Graph input ${in.name} found, but no column to match it. Dataframe columns: $cols")

      val stf = get(ColumnInformation(f).stf,
        s"Data column ${f.name} has not been analyzed yet, cannot run TF on this dataframe")

      val cellShape = stf.shape.tail
      // No check for unknowns: we allow unknowns in the first dimension of the cell shape.
      check(cellShape.checkMorePreciseThan(in.shape),
        s"The data column '${f.name}' has shape ${stf.shape} (not compatible) with shape" +
          s" ${in.shape} requested by the TF graph")

      check(stf.dataType == in.scalarType,
        s"The type of node '${in.name}' (${stf.dataType}) is not compatible with the data type " +
          s"of the column (${in.scalarType})")

      check(in.isPlaceholder,
        s"Invalid type for input node ${in.name}. It has to be a placeholder")
    }

    // The output schema of the block from the data generated by TF.
    val outputTFSchema: StructType = {
      // The order of the output columns is decided for now by their names.
      val fields = outputs.values.toSeq.sortBy(_.name).map { out =>
        check(! fieldsByName.contains(out.name),
          s"TF graph has an output node called '${out.name}'," +
            s" but this column already exists. Input columns: ${cols}")
        // The shapes we get in each output node are the shape of the cells of each column, not the
        // shape of the column. Add Unknown since we do not know the exact length of the block.
        val blockShape = out.shape.prepend(Shape.Unknown)
        ColumnInformation.structField(out.name, out.scalarType, blockShape)
      }
      StructType(fields.toArray)
    }

    // The column indices requested by TF
    val requestedTFInput: Array[Int] = {
      val colIdxs = dataframe.schema.fieldNames.zipWithIndex.toMap
      inputs.keys.map { name => colIdxs(name) }   .toArray
    }
    // Full output schema, including data being passed through and validated for duplicates.
    // The first columns are the TF columns, followed by all the other columns.
    val outputSchema: StructType = {
      StructType(outputTFSchema ++ dataframe.schema.fields)
    }


    val schema = dataframe.schema // Classic rookie mistake...
    logDebug(s"mapRows: input schema = $schema, requested cols: ${requestedTFInput.toSeq}" +
      s" complete output schema = $outputSchema")
    val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))
    val transformRdd = dataframe.rdd.mapPartitions { it =>
      DebugRowOpsImpl.performMapRows(
        it.toArray,
        schema,
        requestedTFInput,
        gProto.value,
        outputTFSchema).toIterator
    }
    dataframe.sqlContext.createDataFrame(transformRdd, outputSchema)
  }

  override def reduceRows(
      dataframe: DataFrame,
      graph: GraphDef,
      shapeHints: ShapeDescription): Row = {
    val sc = dataframe.sqlContext.sparkContext
    // Most of this function is various sanity checks on the graph and on the dataframe.
    reduceRowsSchema(dataframe.schema, graph, shapeHints)
    val schema = dataframe.schema
    val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))
    val transformRdd = dataframe.rdd.mapPartitions { it =>
      // TODO(tjh) add test for empty iterator
      val arr = it.toArray
      if (arr.length <= 1) {
        arr.iterator
      } else {
        val row = DebugRowOpsImpl.performReducePairwise(
          arr, schema, gProto.value
        )
        Array(row).iterator
      }
    }
    transformRdd.reduce(DebugRowOpsImpl.reducePair(schema, gProto))
  }

  override def reduceBlocks(
      dataframe: DataFrame,
      graph: GraphDef,
      shapeHints: ShapeDescription): Row = {
    val sc = dataframe.sqlContext.sparkContext
    val schema = dataframe.schema
    val allSchema = reduceBlocksSchema(schema, graph, shapeHints)
    val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))
    // It first reduces each block, and then performs pair-wise reduction.
    val transformRdd = dataframe.rdd.mapPartitions { it =>
      // TODO(tjh) add test for empty iterator
      val arr = it.toArray
      if (arr.length <= 1) {
        arr.iterator
      } else {
        val row = DebugRowOpsImpl.performReduceBlock(
          arr, allSchema.mapInput, allSchema.mapTFCols, allSchema.output, gProto.value)
        Array(row).iterator
      }
    }
    // TODO: it would be nice to respect the output order here?
    transformRdd.reduce(DebugRowOpsImpl.reducePairBlock(
      allSchema.reduceInput, allSchema.output, gProto))
  }

  override def explain(df: DataFrame): String = {
    val d = explainDetailed(df)

    val builder = new StringBuilder
    builder.append("root\n")
    val prefix = " |"
    d.cols.foreach { col =>
      val f = col.field
      builder.append(s"$prefix-- ${f.name}: ${f.dataType.typeName} (nullable = ${f.nullable})")
      val stf = col.stf.map { s =>
        s" ${s.dataType.typeName}${s.shape}"
      }   .getOrElse(" <no tensor info>")
      builder.append(stf)
      builder.append("\n")
    }
    builder.toString()
  }

  override def aggregate(
      data: RelationalGroupedDataset,
      graph: GraphDef,
      shapeHints: ShapeDescription): DataFrame = {
    // The constraints on the graph are the same as blocked data.
    val dataframe = DebugRowOpsImpl.backingDF(data) match {
      case Success(d) => d
      case Failure(e) =>
        throw e
    }
    logDebug(s"aggregate: found dataframe: $dataframe")

    val sc = dataframe.sqlContext.sparkContext

    val allSchemas = SchemaTransforms.reduceBlocksSchema(
      dataframe.schema, graph, shapeHints)
    val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))

    // TODO(tjh) fix the size of the schema based on the overall size of the row, which we know
    // based on the shapes (there is no unknown shape when reducing).
    // The input schema is a bit different than for reduce: we already filter the columns we are
    // interested in.
    val inputSchema = StructType(allSchemas.output.map { of =>
      allSchemas.mapInput(of.name + "_input")
    })

    val tfudaf = new TensorFlowUDAF(allSchemas.output, inputSchema, gProto, 10)
    val cols = allSchemas.output.fields.map(f => col(f.name))
    val sname = "tf_output"
    val df2 = data.agg(tfudaf(cols: _*).as(sname))
    // Unpack the structure as the main columns:
    // For some reason, the metadata does not get through, so it is added here as a post-processing
    // step.
    // TODO(tjh) add a test for shape propagation
    val unpackCols = allSchemas.output.map { field =>
      val n = field.name
      val meta = ColumnInformation(field).merged.metadata
      logDebug(s"aggregate: output: $n -> $meta")
      col(s"$sname.$n").as(n, meta)
    }

    // The columns that were used for the key
    val othercols = df2.schema.fieldNames.filter(_ != sname).map(n => col(n))
    val allcols = othercols ++ unpackCols
    df2.select(allcols: _*)
  }
}

/**
 * Simple implementation of a reduction with TF graphs.
 *
 * This version keeps a running buffer of elements and compacts them once the buffer becomes too
 * big or once the evaluation is called.
 */
class TensorFlowUDAF(
    val rowSchema: StructType,
    val tfInputSchema: StructType,
    val gProto: Broadcast[Array[Byte]],
    val bufferSize: Int) extends UserDefinedAggregateFunction with Logging {

  private val COUNT = 0
  private val ROWS = 1
  private type RowArray = mutable.WrappedArray[Row]

  override def inputSchema: StructType = rowSchema

  // We keep a buffer of rows, a counter of how many rows we have seen so far, and the current
  // aggregates before operating on the given rows.
  override val bufferSchema: StructType = {
    StructType(List(
      StructField("counter", IntegerType, nullable = false),
      StructField("rows", ArrayType(rowSchema, containsNull = true), nullable = false)
    ))
  }

  override def deterministic: Boolean = true

  override def dataType: DataType = rowSchema

  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(COUNT) = 0
    buffer(ROWS) = empty
  }

  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    val currentCount = count(buffer)
    buffer(COUNT) = currentCount + 1
    val arr = array(buffer)
    logTrace(s"update: arr = $arr")
    arr(currentCount) = input.copy()
    logTrace(s"update: arr2 = $arr")
    buffer(ROWS) = arr
    if (currentCount >= bufferSize) {
      compact(buffer)
    }
  }

  private def compact(buffer: MutableAggregationBuffer): Unit = {
    buffer(COUNT) = 1
    val arr0 = array(buffer)
    val arr1 = empty
    arr1(0) = compact2(arr0)
    buffer(ROWS) = arr1
  }

  private def compact2(arr: mutable.WrappedArray[Row]): Row = {
    // This is simply performing a block reduce on the rows. The schema has already
    DebugRowOpsImpl.performReduceBlock(
      arr.toArray.filterNot(_ == null), tfInputSchema, rowSchema.indices, rowSchema, gProto.value)
  }

  override def merge(buffer: MutableAggregationBuffer, other: Row): Unit = {
    logTrace(s"merge: buffer=$buffer, other=$other")
    val initialCount = count(buffer)
    val otherCount = count(other)
    val (currentCount, totalCount) = if (initialCount + otherCount >= bufferSize) {
      compact(buffer)
      (1, 1 + otherCount)
    } else {
      (initialCount, initialCount + otherCount)
    }
    buffer(COUNT) = totalCount
    val arr = array(buffer)
    val arr2 = array(other)
    for (i <- 0 until otherCount) {
      arr(currentCount + i) = arr2(i).copy()
    }
    buffer(ROWS) = arr
    logTrace(s"merge (after): buffer=$buffer")
  }

  override def evaluate(buffer: Row): Row = {
    logTrace(s"evaluate: $buffer")
    val c = count(buffer)
    require(c >= 1, buffer)
    // No need for compaction
    if (c == 1) {
      array(buffer).head
    } else {
      compact2(array(buffer))
    }
  }

  private def count(b: Row): Int = b.getInt(COUNT)
  private def array(buffer: Row): RowArray = {
    buffer.getAs[RowArray](ROWS)
  }
  private def empty: RowArray = Array.fill[Row](bufferSize + 1)(null)
}

object DebugRowOpsImpl extends Logging {

  /**
   * Accesses the backing dataframe by reflection.
   *
   * This is very brittle, there should be some other ways of doing it.
   *
   * @param groupedData the grouped data
   * @return the dataframe, if it succeeded.
   */
  def backingDF(groupedData: RelationalGroupedDataset): Try[DataFrame] = {
      Try {
        groupedData.getClass.getDeclaredMethods.foreach { m =>
          logDebug(s"method: ${m.getName}")
        }
        val method = groupedData.getClass.getDeclaredMethod("df")
        method.setAccessible(true)
        method.invoke(groupedData).asInstanceOf[DataFrame]
      }   .orElse {
        Try {
          // Find the name of the field, which is tricky...
          val fname = groupedData.getClass
            .getDeclaredFields.find(_.getName.endsWith("df")).getOrElse {
            groupedData.getClass.getDeclaredFields.foreach { m =>
              logDebug(s"field: ${m.getName}")
            }
            throw new Exception("Could not find field")
          }
          val method = groupedData.getClass.getDeclaredField(fname.getName)
          method.setAccessible(true)
          method.get(groupedData).asInstanceOf[DataFrame]
        }
      }
  }

  // Trying to get around some frequent crashes within TF.
  private[this] val tfLock = new Object

  private[impl] def reducePair(
      schema: StructType,
      gbc: Broadcast[Array[Byte]]): (Row, Row) => Row = {
    def f(row1: Row, row2: Row): Row = {
      performReducePairwise(Array(row1, row2), schema, gbc.value)
    }
    f
  }

  def reducePairBlock(
      inputSchema: StructType,
      outputSchema: StructType,
      gbc: Broadcast[Array[Byte]]): (Row, Row) => Row = {
    def f(row1: Row, row2: Row): Row = {
      performReduceBlock(Array(row1, row2), inputSchema, inputSchema.indices.toArray, outputSchema,
        gbc.value)
    }
    f
  }
  /**
   * Performs the data transform. All the data has been verified at this point.
   * 
   * The output format is: all the columns coming out of TF first (their exact content being given
   * by tfOutputSchema), and then all the input columns.
   *
   * @param input the row of input data
   * @param inputSchema the schema of the block of inputs
   * @param inputTFCols the columns of input data that will be converted to TF data and fed into the
   *                    TF session
   * @param graphDef the graph definition that contains the description of the TF graph
   * @param tfOutputSchema the (expected) output schema of the block of outputs
   * @return the array of rows that combine outputs from the TF graph with all the other data
   *         columns.
   */
  def performMap(
      input: Array[Row],
      inputSchema: StructType,
      inputTFCols: Array[Int],
      graphDef: Array[Byte],
      tfOutputSchema: StructType,
      appendInput: Boolean): Iterator[Row] = {
    logDebug(s"performMap: inputSchema=$inputSchema, tfschema=$tfOutputSchema," +
      s" ${input.length} rows, input cols: ${inputTFCols.toSeq}")
    // Some partitions may be empty
    // TODO(tjh) add test for that
    if (input.length == 0) {
      return Iterator.empty
    }
    val stpv = DataOps.convert(input, inputSchema, inputTFCols)
    logDebug(s"performMap: converting the graphDef")
    val g = TensorFlowOps.readGraph(graphDef)
    logDebug(s"performMap: entering session")
    TensorFlowOps.withSession { session =>
      logDebug(s"performMap: entered session session")
      logDebug(s"performMap: extending the graph")
      val s1 = session.Extend(g)
      assert(s1.ok(), s1.error_message().getString)

      val outputs = new jtf.TensorVector()
      val requested = TensorFlowOps.stringVector(tfOutputSchema.map(_.name))
      val skipped = new jtf.StringVector()
      logDebug(s"performMap: waiting for TF lock")
      val s3 = tfLock.synchronized {
        logDebug(s"performMap: TF lock acquired, running...")
        session.Run(stpv, requested, skipped, outputs)
      }
      logDebug(s"performMap: TF run finished")
      assert(s3.ok(), s3.error_message().getString)
      logDebug(s"performMap: converting back")
      val res = DataOps.convertBack(outputs, tfOutputSchema, input, inputSchema, appendInput)
      logDebug(s"performMap: done")
      res
    }
  }

  // For testing only:
  def performMap(
      input: Array[Row],
      inputSchema: StructType,
      inputTFCols: Array[Int],
      graphDef: GraphDef,
      outputSchema: StructType): Array[Row] = {
    // Do a defensive copy of the content of the iterator, as the object may be reused.
    performMap(input, inputSchema, inputTFCols,
      TensorFlowOps.graphSerial(graphDef), outputSchema, appendInput = true).map { row =>
      SerializationUtils.clone(row)
    } .toSeq.toArray
  }

  def performMapRows(
      input: Array[Row],
      inputSchema: StructType,
      inputTFCols: Array[Int],
      graphDef: Array[Byte],
      tfOutputSchema: StructType): Array[Row] = {
    // Some partitions may be empty
    if (input.length == 0) {
      return Array.empty
    }
    // We read the graph once, and within the same session we run each row after the other.
    val g = TensorFlowOps.readGraph(graphDef)
    TensorFlowOps.withSession { session =>
      val s1 = session.Extend(g)
      assert(s1.ok(), s1.error_message().getString)
      val requested = TensorFlowOps.stringVector(tfOutputSchema.map(_.name))

      input.map { row =>
        val stpv = DataOps.convert(row, inputSchema, inputTFCols)
        val outputs = new jtf.TensorVector()
        val skipped = new jtf.StringVector()
        val s3 = tfLock.synchronized { session.Run(stpv, requested, skipped, outputs) }
        assert(s3.ok(), s3.error_message().getString)
        val it = DataOps.convertBack(outputs, tfOutputSchema, Array(row), inputSchema,
          appendInput = true)
        assert(it.hasNext)
        val r = it.next()
        assert(!it.hasNext)
        r
      }
    }
  }

  /**
   * Performs a reduce operation on the set of columns.
   *
   * This performs a fix-point operation around the given schema:
   *  - all the columns are transformed
   *  - for each output X there is a given X_input input. The schema is assumed to be the same with
   *  an extra block dimension.
   *
   * @param input the input of rows.
   * @param schema the output schema.
   * @param graphDef the definition of the graph
   * @return the combined row after transform
   */
  def performReduceBlock(
      input: Array[Row],
      inputSchema: StructType,
      inputTFCols: Seq[Int],
      schema: StructType,
      graphDef: Array[Byte]): Row = {
    logDebug(s"performReduceBlock: schema=$schema inputSchema=$inputSchema with ${input.length} rows")
    // The input schema and the actual data representation depend on the block operation.

    val stpv = DataOps.convert(input, inputSchema, inputTFCols.toArray)
    val g = TensorFlowOps.readGraph(graphDef)
    TensorFlowOps.withSession { session =>
      val s1 = session.Extend(g)
      assert(s1.ok(), s1.error_message().getString)

      val outputs = new jtf.TensorVector()
      val requested = TensorFlowOps.stringVector(schema.map(_.name))
      val skipped = new jtf.StringVector()
      val s3 = tfLock.synchronized { session.Run(stpv, requested, skipped, outputs) }
      assert(s3.ok(), s3.error_message().getString)
      val emptyRows = Array.fill(1)(emptyRow)
      val it = DataOps.convertBack(outputs, schema, emptyRows, emptySchema, appendInput = false)
      assert(it.hasNext)
      val r = it.next()
      r
    }
  }

  private val emptySchema = StructType(Seq.empty)
  private val emptyRow: Row = new GenericRowWithSchema(Array.empty[Any], emptySchema)

  /**
   * Performs a reduce operation on a set of columns by performing pair-wise reductions.
   *
   * @param input
   * @param schema
   * @param graphDef
   * @return
   */
  def performReducePairwise(input: Array[Row], schema: StructType, graphDef: Array[Byte]): Row = {
    require(input.length > 0, "Cannot provide empty input")
    // If there is a single row, no need to perform operations.
    if (input.length == 1) {
      return input.head
    }
    val inputSchema = {
      val f1 = schema.fields.map(f => f.copy(name = s"${f.name}_1"))
      val f2 = schema.fields.map(f => f.copy(name = s"${f.name}_2"))
      StructType(f1 ++ f2)
    }
    // For efficiency, this tries to reuse the session.
    val tfInput: Array[Row] = input.tail
    var result: Row = input.head
    val g = TensorFlowOps.readGraph(graphDef)
    TensorFlowOps.withSession { session =>
      val s1 = session.Extend(g)
      assert(s1.ok(), s1.error_message().getString)
      for (row <- tfInput) {
        val values = (result.toSeq ++ row.toSeq).toArray
        val r: Row = new GenericRowWithSchema(values, inputSchema)
        val thisInput = Array(r)
        val stpv = DataOps.convert(thisInput, inputSchema, inputSchema.fields.indices.toArray)
        val outputs = new jtf.TensorVector()
        val requested = TensorFlowOps.stringVector(schema.map(_.name))
        val skipped = new jtf.StringVector()
        val s3 = tfLock.synchronized { session.Run(stpv, requested, skipped, outputs) }
        assert(s3.ok(), s3.error_message().getString)
        // Fill in with an empty row, because we are not passing the rest of of the data.
        val emptyRows = Array.fill(1)(emptyRow)
        val it = DataOps.convertBack(outputs, schema, emptyRows, emptySchema, appendInput = false)
        assert(it.hasNext)
        result = it.next()
        assert(!it.hasNext)
      }
    }
    result
  }
}

object DebugRowOps extends DebugRowOps
